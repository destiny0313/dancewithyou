{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg \n",
    "import trt_pose.coco\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import traitlets\n",
    "import pickle \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After running this code, go back to above to find game window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import trt_pose.coco\n",
    "\n",
    "with open('preprocess/human_pose.json', 'r') as f:\n",
    "    human_pose = json.load(f)\n",
    "\n",
    "topology = trt_pose.coco.coco_category_to_topology(human_pose)\n",
    "\n",
    "import trt_pose.models\n",
    "\n",
    "num_parts = len(human_pose['keypoints'])\n",
    "num_links = len(human_pose['skeleton'])\n",
    "\n",
    "model = trt_pose.models.resnet18_baseline_att(num_parts, 2 * num_links).cuda().eval()\n",
    "\n",
    "import torch\n",
    "\n",
    "MODEL_WEIGHTS = 'model/resnet18_baseline_att_224x224_A_epoch_249.pth'\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_WEIGHTS))\n",
    "\n",
    "WIDTH = 224\n",
    "HEIGHT = 224\n",
    "\n",
    "data = torch.zeros((1, 3, HEIGHT, WIDTH)).cuda()\n",
    "\n",
    "import torch2trt\n",
    "\n",
    "model_trt = torch2trt.torch2trt(model, [data], fp16_mode=True, max_workspace_size=1<<25)\n",
    "\n",
    "OPTIMIZED_MODEL = 'model/resnet18_baseline_att_224x224_A_epoch_249_trt.pth'\n",
    "\n",
    "torch.save(model_trt.state_dict(), OPTIMIZED_MODEL)\n",
    "\n",
    "from torch2trt import TRTModule\n",
    "\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load(OPTIMIZED_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trt_pose.draw_objects import DrawObjects\n",
    "from trt_pose.parse_objects import ParseObjects\n",
    "\n",
    "parse_objects = ParseObjects(topology)\n",
    "draw_objects = DrawObjects(topology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def preprocess(image):\n",
    "    global device\n",
    "    device = torch.device('cuda')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device)\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto', kernel='rbf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessdata import preprocessdata\n",
    "preprocessdata = preprocessdata(topology, num_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import dataloader\n",
    "path = \"/home/ceng/trt_pose_hand-main/data_collection/\"\n",
    "label_file = \"pose_dataset_train.json\"\n",
    "test_label = \"pose_dataset_test.json\"\n",
    "human = dataloader(path, label_file, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(images):\n",
    "    dist_bn_joints_all_data = []\n",
    "    for im in images:\n",
    "        im = im[:, ::-1, :]\n",
    "        data_im = preprocess(im)\n",
    "        cmap, paf = model_trt(data_im)\n",
    "        cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "        counts, objects, peaks = parse_objects(cmap, paf)\n",
    "        joints = preprocessdata.joints_inference(im, counts, objects, peaks)\n",
    "        dist_bn_joints = preprocessdata.find_distance(joints)\n",
    "        dist_bn_joints_all_data.append(dist_bn_joints)\n",
    "    return dist_bn_joints_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smaller_dataset(dataset, no_samples_per_class, no_of_classes):\n",
    "    total_samples_per_class =10\n",
    "    start = 0\n",
    "    end = no_samples_per_class\n",
    "    new_dataset = []\n",
    "    labels = []\n",
    "    for i in range(no_of_classes):\n",
    "        new_data = dataset[start:end]\n",
    "        start = start+total_samples_per_class\n",
    "        end = start+no_samples_per_class\n",
    "        new_dataset.extend(new_data)\n",
    "        labels.extend([i+1]*no_samples_per_class)\n",
    "    return new_dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, labels_train = human.smaller_dataset(human.train_images,10,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joints_train = data_preprocess(human.train_images)\n",
    "joints_test = data_preprocess(human.test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_train = False\n",
    "if svm_train:\n",
    "    clf, predicted = preprocessdata.trainsvm(clf, joints_train, joints_test, human.labels_train, human.labels_test)\n",
    "    filename = 'svmmodel_new.sav'\n",
    "    pickle.dump(clf, open(filename, 'wb'))\n",
    "else:\n",
    "    filename = 'svmmodel_new.sav'\n",
    "    clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocess/ceng.json', 'r') as f:\n",
    "    gesture = json.load(f)\n",
    "gesture_type = gesture[\"classes\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetcam.usb_camera import USBCamera\n",
    "from jetcam.csi_camera import CSICamera\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "\n",
    "#camera = USBCamera(width=WIDTH, height=HEIGHT, capture_fps=30, capture_device=1)\n",
    "camera = CSICamera(width=WIDTH, height=HEIGHT, capture_fps=30)\n",
    "\n",
    "camera.running = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_joints(image, joints):\n",
    "    count = 0\n",
    "    for i in joints:\n",
    "        if i==[0,0]:\n",
    "            count+=1\n",
    "    if count>= 3:\n",
    "        return \n",
    "    for i in joints:\n",
    "        cv2.circle(image, (i[0],i[1]), 2, (0,0,255), 1)\n",
    "    cv2.circle(image, (joints[0][0],joints[0][1]), 2, (255,0,255), 1)\n",
    "    for i in human_pose['skeleton']:\n",
    "        if joints[i[0]-1][0]==0 or joints[i[1]-1][0] == 0:\n",
    "            break\n",
    "        cv2.line(image, (joints[i[0]-1][0],joints[i[0]-1][1]), (joints[i[1]-1][0],joints[i[1]-1][1]), (0,255,0), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "image_w = ipywidgets.Image(format='jpeg', width=224, height=224)\n",
    "display(image_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pose = False\n",
    "second_pose = False\n",
    "third_pose = False\n",
    "fourth_pose = False\n",
    "fifth_pose = False\n",
    "sixth_pose = False\n",
    "score = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 0\n",
    "score = 0\n",
    "first_flag = False\n",
    "second_flag = False\n",
    "third_flag = False\n",
    "fourth_flag = False\n",
    "fifth_flag = False\n",
    "sixth_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(change):\n",
    "    global time, score, first_flag, second_flag, third_flag, fourth_flag, fifth_flag, sixth_flag\n",
    "    image = change['new']\n",
    "    data = preprocess(image)\n",
    "    cmap, paf = model_trt(data)\n",
    "    cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "    counts, objects, peaks = parse_objects(cmap, paf)\n",
    "    joints = preprocessdata.joints_inference(image, counts, objects, peaks)\n",
    "    draw_joints(image, joints)\n",
    "    #draw_objects(image, counts, objects, peaks)\n",
    "    dist_bn_joints = preprocessdata.find_distance(joints)\n",
    "    gesture = clf.predict([dist_bn_joints,[0]*num_parts*num_parts])\n",
    "    gesture_joints = gesture[0]\n",
    "    preprocessdata.prev_queue.append(gesture_joints)\n",
    "    preprocessdata.prev_queue.pop(0)\n",
    "    \n",
    "        \n",
    "    if(time <= 60):\n",
    "        image = cv2.putText(image, \"Ready\", org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        #-----------------------------------------------\n",
    "        \n",
    "    if(time > 120 and time <= 133):\n",
    "        image = cv2.putText(image, \"4!\", org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        if(preprocessdata.text == \"4\"):\n",
    "            first_flag = True\n",
    "            \n",
    "    if(x > 133 and x <= 138 and first_flag):\n",
    "        if(first_flag):\n",
    "            image = cv2.putText(image, \"Correct!\", org, font, fontScale, (0,255,0), thickness, cv2.LINE_AA)\n",
    "            score = score + 1\n",
    "        else:\n",
    "            image = cv2.putText(image, \"Wrong!\", org, font, fontScale, (0,0,255), thickness, cv2.LINE_AA)\n",
    "            #-----------------------------------------------\n",
    "        \n",
    "        \n",
    "    if(x > 142 and x <= 155):\n",
    "        image = cv2.putText(image, \"5!\", org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        if(preprocessdata.text == \"5\"):\n",
    "            second_flag = True\n",
    "        \n",
    "    if(x > 155 and x <= 160):\n",
    "        if(second_flag):\n",
    "            image = cv2.putText(image, \"Correct!\", org, font, fontScale, (0,255,0), thickness, cv2.LINE_AA)\n",
    "            score = score + 1\n",
    "        else:\n",
    "            image = cv2.putText(image, \"Wrong!\", org, font, fontScale, (0,0,255), thickness, cv2.LINE_AA)\n",
    "            #-----------------------------------------------\n",
    "        \n",
    "    if(x > 168 and x <= 178):\n",
    "        image = cv2.putText(image, \"6!\", org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        if(preprocessdata.text == \"6\"):\n",
    "            third_flag = True\n",
    "        \n",
    "    if(x > 180 and x <= 185):\n",
    "        if(third_flag):\n",
    "            image = cv2.putText(image, \"Correct!\", org, font, fontScale, (0,255,0), thickness, cv2.LINE_AA)\n",
    "            score = score + 1\n",
    "        else:\n",
    "            image = cv2.putText(image, \"Wrong!\", org, font, fontScale, (0,0,255), thickness, cv2.LINE_AA)\n",
    "            #-----------------------------------------------\n",
    "            \n",
    "    if(x > 200 and x <= 210):\n",
    "        image = cv2.putText(image, \"1!\", org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        if(preprocessdata.text == \"1\"):\n",
    "            fourth_flag = True\n",
    "        \n",
    "    if(x > 210 and x <= 215):\n",
    "        if(fourth_flag):\n",
    "            image = cv2.putText(image, \"Correct!\", org, font, fontScale, (0,255,0), thickness, cv2.LINE_AA)\n",
    "            score = score + 1\n",
    "        else:\n",
    "            image = cv2.putText(image, \"Wrong!\", org, font, fontScale, (0,0,255), thickness, cv2.LINE_AA)\n",
    "            #-----------------------------------------------\n",
    "            \n",
    "    if(x > 220 and x <= 230):\n",
    "        image = cv2.putText(image, \"2!\", org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        if(preprocessdata.text == \"2\"):\n",
    "            fifth_flag = True\n",
    "        \n",
    "    if(x > 230 and x <= 235):\n",
    "        if(fifth_flag):\n",
    "            image = cv2.putText(image, \"Correct!\", org, font, fontScale, (0,255,0), thickness, cv2.LINE_AA)\n",
    "            score = score + 1\n",
    "        else:\n",
    "            image = cv2.putText(image, \"Wrong!\", org, font, fontScale, (0,0,255), thickness, cv2.LINE_AA)\n",
    "            #------------------------------------------------\n",
    "            \n",
    "    if(x > 245 and x <= 255):\n",
    "        image = cv2.putText(image, \"3!\", org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        if(preprocessdata.text == \"3\"):\n",
    "            sixth_flag = True\n",
    "        \n",
    "    if(x > 255 and x <= 260):\n",
    "        if(sixth_flag):\n",
    "            image = cv2.putText(image, \"Correct!\", org, font, fontScale, (0,255,0), thickness, cv2.LINE_AA)\n",
    "            score = score + 1\n",
    "        else:\n",
    "            image = cv2.putText(image, \"Wrong!\", org, font, fontScale, (0,0,255), thickness, cv2.LINE_AA)\n",
    "            #-----------------------------------------------\n",
    "            \n",
    "    if(x>270):\n",
    "        score_str = str(score)\n",
    "        score_show = \"Score: \" + score_str\n",
    "        image = cv2.putText(image, score_show, org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "    image_w.value = bgr8_to_jpeg(image[:, ::-1, :])\n",
    "    x = x + 1\n",
    "    \n",
    "    \n",
    "    image_w.value = bgr8_to_jpeg(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face():\n",
    "    import jetson.inference\n",
    "    import jetson.utils\n",
    "    import time\n",
    "\n",
    "    net = jetson.inference.detectNet(\"ssd-mobilenet-v2\", threshold=0.5)\n",
    "    camera = jetson.utils.videoSource(\"csi://0\")      # '/dev/video0' for V4L2\n",
    "    display = jetson.utils.videoOutput(\"display://0\") # 'my_video.mp4' for file\n",
    "\n",
    "    while display.IsStreaming():\n",
    "        img = camera.Capture()\n",
    "        detections = net.Detect(img)\n",
    "        for detection in detections:\n",
    "            if (net.GetClassDesc(detection.ClassID) == \"person\"):\n",
    "                print(\"Game window will start soon...\\nPlease go to the game window cell now.\")\n",
    "                time.sleep(10)\n",
    "                return\n",
    "        display.Render(img)\n",
    "        display.SetStatus(\"Object Detection | Network {:.0f} FPS\".format(net.GetNetworkFPS()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#camera.unobserve_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#camera.running = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
